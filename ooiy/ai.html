<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>AI Photo Edit — Photo Editor</title>
<link rel="preconnect" href="https://cdn.jsdelivr.net">
<style>
  *{box-sizing:border-box;margin:0;padding:0;font-family:'Segoe UI',Tahoma,Arial,sans-serif}
  body{background:linear-gradient(135deg,#081226,#0f1b2e);min-height:100vh;color:#fff;display:flex;flex-direction:column}
  .top-bar{position:fixed;top:0;left:0;right:0;height:60px;background:rgba(0,0,0,.6);backdrop-filter:blur(8px);display:flex;align-items:center;justify-content:space-between;padding:0 1rem;z-index:120}
  .back-btn{background:none;border:none;color:#fff;font-size:1.4rem;cursor:pointer}
  .title{font-size:1.05rem;background:linear-gradient(45deg,#6b9fff,#ff69b4);-webkit-background-clip:text;background-clip:text;-webkit-text-fill-color:transparent}
  .container{margin-top:60px;padding:1rem;display:flex;gap:1rem;align-items:flex-start;justify-content:center}
  .left{width:100%;max-width:820px}
  #previewWrap{background:rgba(255,255,255,0.02);border-radius:12px;padding:12px;display:flex;justify-content:center;align-items:center}
  #preview{max-width:100%;display:block;border-radius:8px}
  .sidebar{width:360px;max-width:40%;min-width:260px}
  .card{background:rgba(255,255,255,0.03);border-radius:10px;padding:12px;margin-bottom:12px}
  textarea{width:100%;height:100px;background:transparent;border:1px solid rgba(255,255,255,0.06);color:#fff;padding:8px;border-radius:6px}
  .et-btn{background:linear-gradient(45deg,#6b9fff,#ff69b4);border:none;color:#fff;padding:8px 12px;border-radius:8px;cursor:pointer}
  .example{background:rgba(255,255,255,0.02);padding:8px;border-radius:6px;margin-top:8px;cursor:pointer}
  .status{color:rgba(255,255,255,0.8);font-size:.9rem;margin-top:8px}
  .note{color:rgba(255,255,255,0.6);font-size:.85rem;margin-top:6px}
</style>
</head>
<body>
  <div class="top-bar">
    <button class="back-btn" onclick="history.back()">←</button>
    <div class="title">AI Edit</div>
    <div style="width:24px"></div>
  </div>

  <div class="container">
    <div class="left">
      <div id="previewWrap" class="card">
        <canvas id="canvas" style="max-width:100%;width:100%;height:auto;border-radius:6px"></canvas>
      </div>
      <div class="note">This page performs client-side edits. It uses TensorFlow.js for convolutional edits (blur/sharpen/edge) and canvas filters for color/contrast adjustments. No keys or remote APIs required.</div>
    </div>

    <div class="sidebar">
      <div class="card">
        <label for="prompt">Edit prompt</label>
        <textarea id="prompt" placeholder="e.g. Make it brighter and more vibrant, add warm tones, slight sharpen"></textarea>
        <div style="display:flex;gap:.5rem;margin-top:.6rem">
          <button class="et-btn" id="runBtn">Run</button>
          <button class="et-btn" id="saveBtn">Save</button>
        </div>
        <div class="status" id="status">Ready</div>
      </div>

      <div class="card">
        <div style="font-weight:600">Examples</div>
        <div class="example" data-ex="brighten, increase saturation and warmth">brighten, increase saturation and warmth</div>
        <div class="example" data-ex="soft blur and vignette">soft blur and vignette</div>
        <div class="example" data-ex="sharpen and boost contrast">sharpen and boost contrast</div>
        <div class="example" data-ex="grayscale with strong contrast">grayscale with strong contrast</div>
      </div>

      <div class="card">
        <div style="font-weight:600">Notes</div>
        <div class="note">Prompts are interpreted client-side. Supported edits: brightness, contrast, saturation, warmth/cool, sepia, invert, blur, sharpen, vignette, grayscale. Blur/sharpen run via TensorFlow.js convolution for better quality.</div>
      </div>
    </div>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.9.0/dist/tf.min.js"></script>
  <script>
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    const status = document.getElementById('status');
    const promptEl = document.getElementById('prompt');
    const runBtn = document.getElementById('runBtn');
    const saveBtn = document.getElementById('saveBtn');

    let baseImg = new Image();
    let currentImageDataUrl = null;

    // load image from localStorage (same key used by editor)
    window.addEventListener('load', ()=>{
      const data = localStorage.getItem('editImage');
      if(!data){ status.textContent = 'No image found in editor. Open editor and load an image first.'; return; }
      currentImageDataUrl = data;
      baseImg.src = data;
      baseImg.onload = ()=>{ drawBase(); };
    });

    function drawBase(){
      canvas.width = baseImg.naturalWidth; canvas.height = baseImg.naturalHeight;
      ctx.clearRect(0,0,canvas.width,canvas.height);
      ctx.drawImage(baseImg, 0,0, canvas.width, canvas.height);
    }

    // simple prompt parser -> action object
    function parsePrompt(text){
      const t = text.toLowerCase();
      const actions = {
        brightness: 0,
        contrast: 0,
        saturation: 0,
        hue: 0, // degrees
        sepia: false,
        invert: false,
        grayscale: false,
        vignette: false,
        convolution: null // 'blur' or 'sharpen' or 'edge'
      };

      if(/bright|lighter|increase brightness/.test(t)) actions.brightness = 0.12;
      if(/dark|darker|decrease brightness|underexposed/.test(t)) actions.brightness = -0.12;
      if(/contrast|boost contrast|strong contrast|boost|punchy/.test(t)) actions.contrast = 0.18;
      if(/less contrast|reduce contrast/.test(t)) actions.contrast = -0.18;
      if(/vibrant|saturat|more saturated|increase saturation/.test(t)) actions.saturation = 0.3;
      if(/desaturat|muted|less saturated/.test(t)) actions.saturation = -0.3;
      if(/warm|warmth|golden|golden hour/.test(t)) actions.hue = -10; // slight warm shift
      if(/cool|cooler|blue|teal/.test(t)) actions.hue = 20;
      if(/sepia/.test(t)) actions.sepia = true;
      if(/invert|negative/.test(t)) actions.invert = true;
      if(/gray|grayscale|black and white|b&w/.test(t)) actions.grayscale = true;
      if(/vignette/.test(t)) actions.vignette = true;
      if(/blur|soft|smooth|dreamy/.test(t)) actions.convolution = 'blur';
      if(/sharpen|sharper|clarity/.test(t)) actions.convolution = 'sharpen';
      if(/edge|edge detect|outline/.test(t)) actions.convolution = 'edge';

      // intensity modifiers
      const strong = /strong|heavy|intense/.test(t);
      if(strong){ actions.brightness *= 1.6; actions.contrast *= 1.6; actions.saturation *= 1.6; }

      return actions;
    }

    // apply canvas-level filters (brightness/contrast/saturation/hue/sepia/invert/grayscale/vignette)
    function applyCanvasFilters(actions){
      // Build CSS filter string for canvas; use a conservative mapping
      const filters = [];
      if(actions.brightness) filters.push(`brightness(${1 + actions.brightness})`);
      if(actions.contrast) filters.push(`contrast(${1 + actions.contrast})`);
      if(actions.saturation) filters.push(`saturate(${1 + actions.saturation})`);
      if(actions.hue) filters.push(`hue-rotate(${actions.hue}deg)`);
      if(actions.sepia) filters.push(`sepia(0.6)`);
      if(actions.invert) filters.push(`invert(1)`);
      if(actions.grayscale) filters.push(`grayscale(1)`);

      // draw original into temp canvas and then copy using filter
      const tmp = document.createElement('canvas'); tmp.width = canvas.width; tmp.height = canvas.height;
      const tctx = tmp.getContext('2d');
      tctx.filter = filters.join(' ');
      tctx.drawImage(canvas, 0,0);
      // optional vignette overlay
      if(actions.vignette){ tctx.globalCompositeOperation = 'source-over'; const g = tctx.createRadialGradient(tmp.width/2,tmp.height/2, Math.min(tmp.width,tmp.height)*0.2, tmp.width/2,tmp.height/2, Math.max(tmp.width,tmp.height)*0.8); g.addColorStop(0,'rgba(0,0,0,0)'); g.addColorStop(1,'rgba(0,0,0,0.6)'); tctx.fillStyle = g; tctx.globalCompositeOperation = 'multiply'; tctx.fillRect(0,0,tmp.width,tmp.height); tctx.globalCompositeOperation = 'source-over'; }

      // copy back to main canvas
      ctx.clearRect(0,0,canvas.width,canvas.height);
      ctx.drawImage(tmp,0,0);
    }

    // run tfjs convolution (blur/sharpen/edge)
    async function runConvolution(actions){
      if(!actions.convolution) return;
      status.textContent = 'Running TensorFlow convolution...';
      await tf.ready();

      const imgTensor = tf.browser.fromPixels(canvas).toFloat().div(255).expandDims(0); // [1,h,w,3]
      let kernel;

      if(actions.convolution === 'sharpen'){
        // sharpen kernel
        const k = [0, -1, 0, -1, 5, -1, 0, -1, 0];
        kernel = tf.tensor4d(k, [3,3,1,1]);
      } else if(actions.convolution === 'edge'){
        const k = [-1,-1,-1,-1,8,-1,-1,-1,-1];
        kernel = tf.tensor4d(k, [3,3,1,1]);
      } else { // blur
        const k = [1,2,1,2,4,2,1,2,1]; // separable gaussian approx
        kernel = tf.tensor4d(k, [3,3,1,1]).div(tf.scalar(16));
      }

      // expand kernel to apply to 3 channels (inChannels=3, outChannels=3), by stacking
      const kArr = await kernel.array(); // shape [3,3,1,1]
      // build full kernel values repeated across 3 in/out channels
      const fullVals = [];
      for(let i=0;i<3;i++){ // kh
        for(let j=0;j<3;j++){
          for(let inC=0; inC<3; inC++){
            for(let outC=0; outC<3; outC++){
              // use same scalar for each channel mapping
              fullVals.push(kArr[i][j][0][0]);
            }
          }
        }
      }
      const fullKernel = tf.tensor4d(fullVals, [3,3,3,3]);

      const conv = tf.conv2d(imgTensor, fullKernel, 1, 'same'); // [1,h,w,3]
      // clamp and convert back
      const clamped = conv.clipByValue(0,1).squeeze().mul(255).toInt();

      // draw to canvas via tf.browser.toPixels
      await tf.browser.toPixels(clamped, canvas);

      // dispose
      imgTensor.dispose(); kernel.dispose(); fullKernel.dispose(); conv.dispose(); clamped.dispose();
      status.textContent = 'Convolution complete';
    }

    // main run handler
    runBtn.addEventListener('click', async ()=>{
      if(!baseImg.src){ status.textContent = 'No base image loaded.'; return; }
      status.textContent = 'Parsing prompt...';
      drawBase();
      const actions = parsePrompt(promptEl.value || '');
      status.textContent = 'Applying edits...';

      // If convolution requested, run convolution first (operates on current canvas)
      if(actions.convolution){
        // we draw base first (done) and then convolution
        await runConvolution(actions);
        // after convolution, apply canvas filters if any remain
        // zero out convolution to avoid repeating
        const conv = actions.convolution; actions.convolution = null;
        // remove convolution-specific flag
        if(actions.brightness || actions.contrast || actions.saturation || actions.hue || actions.sepia || actions.invert || actions.grayscale || actions.vignette){ applyCanvasFilters(actions); }
      } else {
        // just canvas filters
        applyCanvasFilters(actions);
      }

      status.textContent = 'Done. You can Save to replace the image in the editor.';
    });

    // save back to localStorage
    saveBtn.addEventListener('click', ()=>{
      try{
        const out = canvas.toDataURL('image/png');
        localStorage.setItem('editImage', out);
        saveBtn.textContent = 'Saved ✓';
        setTimeout(()=> saveBtn.textContent = 'Save', 1000);
      }catch(e){ console.error(e); alert('Failed to save image: '+e.message); }
    });

    // example click
    document.querySelectorAll('.example').forEach(el=> el.addEventListener('click', ()=>{ promptEl.value = el.dataset.ex; }));
  </script>
</body>
</html>
